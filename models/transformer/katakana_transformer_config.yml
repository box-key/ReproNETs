## sample path
save_data: opennmt/katakana/run/sample

## vocab path
src_vocab: opennmt/katakana/run/vocab.src
tgt_vocab: opennmt/katakana/run/vocab.tgt
overwrite: True
share_vocab: False
# n_sample: 15811

## data path
data:
  corpus:
    path_src: opennmt/katakana/datasets/src-train.txt
    path_tgt: opennmt/katakana/datasets/tgt-train.txt
    transforms: []
    weight: 1
  valid:
    path_src: opennmt/katakana/datasets/src-val.txt
    path_tgt: opennmt/katakana/datasets/tgt-val.txt
    transforms: []

## training parameters
save_model: opennmt/katakana/run/models
save_checkpoint_steps: 500 ## save a model every this iterations
train_steps: 5000 ## epochs
valid_steps: 500 ## Run evaluation every this many iterations. (from official docs)
report_every: 500
seed: 42

## batch parameters
## More informaiton: https://opennmt.net/OpenNMT-py/options/train.html
batch_type: "tokens"
bucket_size: 15811
batch_size: 4096
# valid_batch_size: 8
max_generator_batches: 0
accum_count: [4]
accum_steps: [0]
gpu_ranks: [0]

## Optimizer
model_dtype: "fp32"
optim: "adam"
learning_rate: 2
warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 2
dec_layers: 2
heads: 8
rnn_size: 512
src_word_vec_size: 512
tgt_word_vec_size: 512
transformer_ff: 2048 ## Size of hidden transformer feed-forward
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
